---
title: "Covid_19"
author: "Gerardo Goar, Oliver Tausendsch√∂n"
date: "2024-11-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Abstract



# Introduction


# Setup
```{r}
library(dplyr)
library(glmnet)

```



```{r}
data<-read.csv("/home/oliver/Documents/Statisticcal_Modelling_Project/Data/covid_data.csv")
```


```{r}
head(data)
```
Drop Columns that have information about Covid Included.

```{r}
covid_19 <- data %>%
  select(
    -c(
      "new_cases", "new_cases_smoothed", "total_deaths", "new_deaths", "new_deaths_smoothed",
      "total_cases_per_million", "new_cases_per_million", "new_cases_smoothed_per_million",
      "total_deaths_per_million", "new_deaths_per_million", "new_deaths_smoothed_per_million",
      "reproduction_rate", "icu_patients", "icu_patients_per_million", "hosp_patients",
      "hosp_patients_per_million", "weekly_icu_admissions", "weekly_icu_admissions_per_million",
      "weekly_hosp_admissions", "weekly_hosp_admissions_per_million", "total_tests", "new_tests",
      "total_tests_per_thousand", "new_tests_per_thousand", "new_tests_smoothed",
      "new_tests_smoothed_per_thousand", "positive_rate", "tests_per_case", "tests_units",
      "total_vaccinations", "people_vaccinated", "people_fully_vaccinated", "total_boosters",
      "new_vaccinations", "new_vaccinations_smoothed", "total_vaccinations_per_hundred",
      "people_vaccinated_per_hundred", "people_fully_vaccinated_per_hundred",
      "total_boosters_per_hundred", "new_vaccinations_smoothed_per_million",
      "new_people_vaccinated_smoothed", "new_people_vaccinated_smoothed_per_hundred"
    ),
    -starts_with("excess")
  )
```



```{r}
# Filter dataset and select the first date when total_cases is at least 1
initial_cases <- covid_19 %>%
  filter(total_cases >= 1) %>%
  group_by(location) %>%
  arrange(date) %>%
  slice(1) %>%   # Keep only the first observation per country
  ungroup()

# View the cleaned dataset
head(initial_cases)
```

```{r}
# Define the percentage threshold
threshold_pct <- 20  # Set percentage threshold, e.g., 20%
threshold <- ncol(initial_cases) * threshold_pct / 100  # Calculate the number of NAs corresponding to the percentage
cat("original amount of Na:", sum(is.na(initial_cases)))
# Step 1: Remove rows with more missing values than the threshold
initial_cases <- initial_cases %>%
  filter(apply(., 1, function(x) sum(is.na(x)) <= threshold))  # Use apply to check each row

# Step 2: Impute remaining NAs with the median of each column
initial_cases <- initial_cases %>%
  mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
cat("\nafter imputation amount of Na:", sum(is.na(initial_cases)))

# View the cleaned and imputed data
print(initial_cases)

```



```{r}

# Group by country, filter for the maximum total cases, and select the latest date if tied
max_date <- covid_19 %>%
  group_by(location) %>%
  arrange(desc(date)) %>% 
  #filter(total_cases == max(total_cases, na.rm = TRUE)) %>%
  #arrange(desc(date)) %>% 
  slice(1) %>% # Keep only the latest observation per country in case of ties
  ungroup()

# View the resulting dataset
head(max_date)

```

Merging these dataframes:

```{r}

covid_19<-merge(initial_cases, max_date[, c("location", "total_cases")], by.x = "location", by.y = "location")

#covid_19 <- covid_19 %>%
#  select(-total_cases.x, -date)
# Create dummy variables manually
covid_19 <- covid_19 %>%
  mutate(
    continent_Africa = ifelse(continent == "Africa", 1, 0),
    continent_Asia = ifelse(continent == "Asia", 1, 0),
    continent_Europe = ifelse(continent == "Europe", 1, 0),
    continent_North_America = ifelse(continent == "North America", 1, 0),
    continent_Oceania = ifelse(continent == "Oceania", 1, 0),
    continent_South_America = ifelse(continent == "South America", 1, 0)
  ) %>%
  select(-continent, -total_cases.x, -date)  # Optionally, remove the original 'continent' column

# View the resulting dataframe
head(covid_19)
str(covid_19)
```



```{r}
# Assuming your dataset is named df

# Step 1: Select columns 7 to 21 for interaction terms
columns_to_exclude <- c("location", "iso_code", "total_cases.y", 
                        "continent_Africa", "continent_Asia", "continent_Europe", 
                        "continent_North_America", "continent_Oceania", "continent_South_America")
covid_19_temp <- covid_19 %>%
  select(-one_of(columns_to_exclude))
#df_selected <- covid_data_model[, c(-1, -2, -19, -20, -21, -22,-23, -24, -25)]  # Select columns 7 to 21

# Step 2: Create pairwise interactions
covid_19_temp <- model.matrix(~ .^2, data = covid_19_temp)  # This generates pairwise interactions

# Step 3: Create three-way interactions
#df_three_way <- model.matrix(~ .^3, data = df_selected)  # This generates pairwise + three-way interactions

# Step 4: Combine the interactions with the original dataset (if needed)
#df_with_interactions <- cbind(df, df_pairwise[, -1], df_three_way[, -(1:ncol(df_selected))])
covid_19 <- cbind(covid_19, covid_19_temp[, -1])  # Remove the intercept column
# Select the columns you want to append from the original dataset
#columns_to_append <- covid_data_model[, c(1, 2, 19, 20, 21, 22, 23, 24, 25)]

# Combine them with df_with_interactions
#df_with_all_columns <- cbind(df_with_interactions, columns_to_append)
#df_with_all_columns<-df_with_all_columns[, -1]
```




```{r}
covid_19_countries<-covid_19 #safe covid_19 with countries
covid_19 <- covid_19[, !names(covid_19) %in% c("location", "iso_code")]

covid_19 <- covid_19 %>%
  mutate(across(where(is.character), as.numeric))  # Convert character columns to numeric
```

# MLE
```{r}
fit.mle <- lm(covid_19$total_cases.y ~., data = covid_19) #1st column in x is the intercept, automatically added by lm
b.mle <- coef(fit.mle)
summary(fit.mle)
```

# Lasso

We use function `cv.glmnet` to select a $\lambda$ via 10-fold cross-validation. The intercept is automatically included, hence we use `x[,-1]` to exclude the first column from `x` (that containing the intercept). `alpha` is by default set at 1, so we are fitting a LASSO regression. Of all the tried values for $\lambda$ `cv.glmnet` outputs two "best" ones.

```{r}
y <- covid_19$total_cases.y

print(sum(is.na(y)))
x <- covid_19[, names(covid_19) != "total_cases.y"]
fit.lasso <- cv.glmnet(x = x, y = y, nfolds = 10) #SHOULD WE EXCLUDE THE INTERCEPT LIKE IN SEMINAR1
summary(fit.lasso)
```

```{r}
fit.mle <- lm(y ~x) #1st column in x is the intercept, automatically added by lm
fit.mle <- lm(y ~ x]) #1st column in x is the intercept, automatically added by lm

```

We can plot the estimated mean squared prediction error $\widehat{MSPE}$ against the tried values of $\lambda$ (in logarithmic scale). Since $\widehat{MSPE}$ is a data-based estimate, we also plot its standard errors. At the top we see the number of coefficients estimated nonzero for the different values of $\lambda$.

```{r}
plot(fit.lasso)
```

We now switch to LASSO regression.We will use the `glmnet` package that implements LASSO, Ridge and ElasticNet
penalizations for linear regression with Gaussian errors and generalized
linear models (logistic, multinomial, Poisson, Cox).


# Data preperation



## Setup

